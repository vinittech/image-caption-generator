{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The Image Captioning Task is divided into 5 sub-tasks. \n",
    "\n",
    "- __DataPreprocessing__ - For this task, we use Flickr 8k Dataset, which is convenient to setup and train on our local machine. Here, we extract descriptions.txt file which contains all the captions mapped with the corresponding Image IDs. The code is found in the file (Part1_Data-Preprocess.ipynb)  \n",
    "\n",
    "\n",
    "- __Extracting Features__ - For the Flickr 8K Image dataset, we use InceptionV3 pretrained model to encode the image features(train + test images) and dump into a featuresNew.pkl file. The code is found in the file (Part2_Features-Extract.ipynb)\n",
    "\n",
    "\n",
    "- __Training the model__ - The LSTM model is built using Keras API. The model (CNN + LSTM) is trained for 10 epochs. However, since we already have the CNN features extracted, only the LSTM model is trained. The code is found in the file (Part3_Training_Notebook.ipynb)\n",
    "\n",
    "\n",
    "- __Evaluate the training process__ - The training history is plotted here, specifically how the loss decreases during the training. The code is found in the file (Part4_Plotloss.ipynb)\n",
    "\n",
    "\n",
    "- __Generating Captions__ - Finally, this is the last step of the captioning task where we generate captions using our trained LSTM model. The captions are generated on test images. The code is found in the file (Part5_Generate_Caption_testSet.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'Z:/Flickr_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a file\n",
    "\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns mapping of Image_ID and Image_Captions\n",
    "\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    # process lines\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        \n",
    "        if not line:\n",
    "            continue \n",
    "        image_id = tokens.pop(0)\n",
    "        image_desc = tokens[:]\n",
    "\n",
    "        # remove filename from image id\n",
    "        image_id = image_id.split('.').pop(0)\n",
    "        image_id = image_id + '.jpg'\n",
    "        \n",
    "        # convert description tokens back to string\n",
    "        image_desc = ' '.join(map(str,image_desc))\n",
    "        \n",
    "        # create the list if needed\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        # store description\n",
    "        mapping[image_id].append(image_desc)\n",
    "          \n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary from all the captions\n",
    "\n",
    "def to_vocabulary(descriptions):\n",
    "    # build a list of all description strings\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Image ID with the corresponding captions (contains both train and test image IDs)\n",
    "\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for _ ,(key, desc_list) in enumerate(descriptions.items()):\n",
    "        _ = [lines.append(key + ' ' + desc) for desc in desc_list]\n",
    "    data = '\\n'.join(map(str,lines))\n",
    "    with open(filename, 'w') as file:\n",
    "        text = file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n",
      "Vocabulary Size: 9630\n"
     ]
    }
   ],
   "source": [
    "filename = 'Flickr_Data/Flickr_TextData/Flickr8k.token.txt'\n",
    "# load descriptions\n",
    "doc = load_doc(filename)\n",
    "# parse descriptions\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "\n",
    "\n",
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "# save to file\n",
    "save_descriptions(descriptions, 'save/descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9632\n"
     ]
    }
   ],
   "source": [
    "vocab=list(vocabulary)\n",
    "vocab.append(\"<start>\")\n",
    "vocab.append(\"<end>\")\n",
    "print(len(vocab))\n",
    "pickle.dump( vocab, open( \"save/vocab.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating captions for training, captions appended with <start> and <end> sequences.\n",
    "\n",
    "def generate_captions(fileName,dataset):\n",
    "    \n",
    "    imgs_captions = open(fileName,'w')\n",
    "    \n",
    "    dataset = open(dataset).read().split('\\n')[:-1]\n",
    "         \n",
    "    start = \"<start> \"\n",
    "    end = \" <end>\"\n",
    "    \n",
    "    for img_id in dataset:\n",
    "        for caption in descriptions[img_id]:\n",
    "\n",
    "            full_caption = start + caption + end\n",
    "            imgs_captions.write(img_id+\"\\t\"+full_caption+\"\\n\")\n",
    "            imgs_captions.flush()\n",
    "        \n",
    "    imgs_captions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating captions(ID, caption) for training - 6000 Images\n",
    "\n",
    "train_imgs_captions = \"save/trainCaptions.txt\"\n",
    "train_imgs_id = \"features/Flickr_8k.trainImages.txt\"\n",
    "\n",
    "generate_captions(train_imgs_captions,train_imgs_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating captions(ID, caption) for testing - 6000 Images\n",
    "\n",
    "test_imgs_captions = \"save/testCaptions.txt\"\n",
    "test_imgs_id = \"features/Flickr_8k.testImages.txt\"\n",
    "\n",
    "generate_captions(test_imgs_captions,test_imgs_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
